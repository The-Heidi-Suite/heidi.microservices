# Docker Compose configuration for HEIDI microservices
#
# ⚠️  DATA PERSISTENCE ⚠️
# All infrastructure data is stored in ./data/ directory using bind mounts.
# This means:
# - Data persists when containers are stopped (docker compose down)
# - Data is visible and accessible on your host machine
# - Easy to backup: just copy the ./data/ folder
# - Safe from accidental docker volume prune operations
#
# BUILD MEMORY OPTIMIZATION:
# If you encounter "cannot allocate memory" errors during builds:
# 1. Use sequential builds: yarn docker:build:sequential (or npm run docker:build:sequential)
# 2. Increase Docker Desktop memory allocation (Settings > Resources > Memory)
# 3. Build services individually: docker compose build <service-name>
#
services:
  postgres:
    image: postgres:16-alpine
    container_name: heidi-postgres
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-heidi}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-heidi_password}
      POSTGRES_DB: ${POSTGRES_DB:-heidi_db}
    ports:
      - '${POSTGRES_PORT:-5432}:5432'
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
      - ./infra/postgres/init-databases.sh:/docker-entrypoint-initdb.d/init-databases.sh:ro
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U ${POSTGRES_USER:-heidi}']
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - backend

  redis:
    image: redis:7.4-alpine
    container_name: heidi-redis
    restart: always
    command: >
      sh -c "if [ -n \"$$REDIS_PASSWORD\" ]; then
        redis-server --requirepass \"$$REDIS_PASSWORD\";
      else
        redis-server;
      fi"
    ports:
      - '${REDIS_PORT:-6379}:6379'
    volumes:
      - ./data/redis:/data
    healthcheck:
      test: ['CMD-SHELL', 'if [ -n "$$REDIS_PASSWORD" ]; then redis-cli -a "$$REDIS_PASSWORD" ping; else redis-cli ping; fi']
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - backend

  rabbitmq:
    image: rabbitmq:4.1.5-management-alpine
    container_name: heidi-rabbitmq
    restart: always
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_USER:-heidi}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASSWORD:-heidi_password}
      RABBITMQ_DEFAULT_VHOST: ${RABBITMQ_VHOST:-/}
    ports:
      - '${RABBITMQ_PORT:-5672}:5672'
      - '${RABBITMQ_MANAGEMENT_PORT:-15672}:15672'
      - '${RABBITMQ_PROMETHEUS_PORT:-15692}:15692'
    volumes:
      - ./data/rabbitmq:/var/lib/rabbitmq
      - ./infra/rabbitmq/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf:ro
    healthcheck:
      test: ['CMD', 'rabbitmq-diagnostics', 'ping']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - backend

  # Reverse Proxy with Auto SSL: Caddy (Default for Development)
  caddy:
    image: caddy:2.7-alpine
    container_name: heidi-caddy
    restart: always
    env_file:
      - .env
    environment:
      DEV_IP: ${DEV_IP:-localhost}
      LETSENCRYPT_EMAIL: ${LETSENCRYPT_EMAIL:-admin@heidi.com}
      LETSENCRYPT_STAGING: ${LETSENCRYPT_STAGING:-false}
    command: >
      sh -c "
        apk add --no-cache gettext &&
        envsubst < /etc/caddy/Caddyfile > /tmp/Caddyfile &&
        caddy run --config /tmp/Caddyfile
      "
    ports:
      - '${CADDY_HTTP_PORT:-80}:80'
      - '${CADDY_HTTPS_PORT:-443}:443'
      - '${CADDY_HTTPS_PORT:-443}:443/udp'
    volumes:
      - ./infra/caddy/Caddyfile:/etc/caddy/Caddyfile:ro
      - ./data/caddy:/data
      - ./data/caddy-config:/config
      # Mount React build folder (set REACT_BUILD_PATH in .env)
      - ${REACT_BUILD_PATH:-../heidi-frontend/build}:/var/www/frontend:ro
    networks:
      - backend
    depends_on:
      - auth
      - users
      - city
      - core
      - notification
      - scheduler
      - integration
      - admin
    healthcheck:
      test: ['CMD-SHELL', 'wget --spider -q http://localhost:2019/metrics || curl -f http://localhost:2019/metrics || exit 1']
      interval: 30s
      timeout: 10s
      retries: 3

  # Metrics Exporter: PostgreSQL
  postgres-exporter:
    profiles: ["monitoring"]
    image: quay.io/prometheuscommunity/postgres-exporter:latest
    container_name: heidi-postgres-exporter
    restart: always
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER:-heidi}:${POSTGRES_PASSWORD:-heidi_password}@postgres:5432/postgres?sslmode=disable"
      PG_EXPORTER_EXTEND_QUERY_PATH: /etc/postgres_exporter/queries.yaml
    ports:
      - '${POSTGRES_EXPORTER_PORT:-9187}:9187'
    networks:
      - backend
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ['CMD-SHELL', 'wget --spider -q http://localhost:9187/metrics || curl -f http://localhost:9187/metrics || exit 1']
      interval: 30s
      timeout: 10s
      retries: 3

  # Metrics Exporter: Redis
  redis-exporter:
    profiles: ["monitoring"]
    image: oliver006/redis_exporter:latest
    container_name: heidi-redis-exporter
    restart: always
    environment:
      REDIS_ADDR: "redis:6379"
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
    ports:
      - '${REDIS_EXPORTER_PORT:-9121}:9121'
    networks:
      - backend
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ['CMD-SHELL', 'wget --spider -q http://localhost:9121/metrics || curl -f http://localhost:9121/metrics || exit 1']
      interval: 30s
      timeout: 10s
      retries: 3

  # Metrics Exporter: RabbitMQ (using built-in Prometheus endpoint)
  # Note: RabbitMQ 4.x has built-in Prometheus metrics on port 15692
  # This is already configured in rabbitmq.conf and exposed above

  # Metrics Exporter: Node (Host System Metrics)
  node-exporter:
    profiles: ["monitoring"]
    image: prom/node-exporter:latest
    container_name: heidi-node-exporter
    restart: always
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    ports:
      - '${NODE_EXPORTER_PORT:-9100}:9100'
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    networks:
      - backend
    healthcheck:
      test: ['CMD-SHELL', 'wget --spider -q http://localhost:9100/metrics 2>/dev/null || curl -sf http://localhost:9100/metrics >/dev/null || (echo >/dev/tcp/localhost/9100) || exit 1']
      interval: 30s
      timeout: 10s
      retries: 3

  # Monitoring: Prometheus
  prometheus:
    profiles: ["monitoring"]
    image: prom/prometheus:latest
    container_name: heidi-prometheus
    restart: always
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    ports:
      - '${PROMETHEUS_PORT:-9090}:9090'
    volumes:
      - ./infra/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./infra/prometheus/alerts:/etc/prometheus/alerts:ro
      - ./data/prometheus:/prometheus
    networks:
      - backend
    depends_on:
      - postgres-exporter
      - redis-exporter
      - node-exporter
      - rabbitmq
    healthcheck:
      test: ['CMD-SHELL', 'wget --spider -q http://localhost:9090/-/healthy || curl -f http://localhost:9090/-/healthy || exit 1']
      interval: 30s
      timeout: 10s
      retries: 3

  # Monitoring: Alertmanager
  alertmanager:
    profiles: ["monitoring"]
    image: prom/alertmanager:latest
    container_name: heidi-alertmanager
    restart: always
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://localhost:${ALERTMANAGER_PORT:-9093}'
    ports:
      - '${ALERTMANAGER_PORT:-9093}:9093'
    volumes:
      - ./infra/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - ./data/alertmanager:/alertmanager
    networks:
      - backend
    depends_on:
      - prometheus
    healthcheck:
      test: ['CMD-SHELL', 'wget --spider -q http://localhost:9093/-/healthy || curl -f http://localhost:9093/-/healthy || exit 1']
      interval: 30s
      timeout: 10s
      retries: 3

  # Monitoring: Grafana
  grafana:
    profiles: ["monitoring"]
    image: grafana/grafana:latest
    container_name: heidi-grafana
    restart: always
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-admin}
      GF_USERS_ALLOW_SIGN_UP: ${GRAFANA_ALLOW_SIGN_UP:-false}
      GF_SERVER_ROOT_URL: ${GRAFANA_ROOT_URL:-http://localhost:${GRAFANA_PORT:-3000}}
    ports:
      - '${GRAFANA_PORT:-3000}:3000'
    volumes:
      - ./data/grafana:/var/lib/grafana
      - ./infra/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./infra/grafana/provisioning/dashboards:/var/lib/grafana/dashboards:ro
    networks:
      - backend
    depends_on:
      - prometheus
    healthcheck:
      test: ['CMD-SHELL', 'wget --spider -q http://localhost:3000/api/health || curl -f http://localhost:3000/api/health || exit 1']
      interval: 30s
      timeout: 10s
      retries: 3

  # Database Admin: pgAdmin
  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: heidi-pgadmin
    restart: always
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_EMAIL:-admin@heidi.com}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_PASSWORD:-admin}
      PGADMIN_CONFIG_SERVER_MODE: 'False'
    ports:
      - '${PGADMIN_PORT:-5050}:80'
    volumes:
      - ./data/pgadmin:/var/lib/pgadmin
      - ./infra/pgadmin/fix-permissions.sh:/fix-permissions.sh:ro
    networks:
      - backend
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ['CMD-SHELL', 'wget --spider -q http://localhost/misc/ping || curl -f http://localhost/misc/ping || exit 1']
      interval: 30s
      timeout: 10s
      retries: 3
    # Fix permissions on startup - pgAdmin runs as UID 5050
    entrypoint: ["/bin/sh", "/fix-permissions.sh"]

  # Redis Admin: Redis Commander
  redis-commander:
    image: rediscommander/redis-commander:latest
    container_name: heidi-redis-commander
    restart: always
    environment:
      REDIS_HOSTS: local:redis:6379
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
      HTTP_USER: ${REDIS_COMMANDER_USER:-admin}
      HTTP_PASSWORD: ${REDIS_COMMANDER_PASSWORD:-admin}
    ports:
      - '${REDIS_COMMANDER_PORT:-8081}:8081'
    networks:
      - backend
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ['CMD-SHELL', 'wget --spider -q http://localhost:8081/health || curl -f http://localhost:8081/health || exit 1']
      interval: 30s
      timeout: 10s
      retries: 3

  # Reverse Proxy: Nginx (Production)
  nginx:
    profiles: ["production"]
    image: nginx:alpine
    container_name: heidi-nginx
    restart: always
    ports:
      - '${NGINX_HTTP_PORT:-80}:80'
      - '${NGINX_HTTPS_PORT:-443}:443'
    volumes:
      - ./infra/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./infra/nginx/ssl:/etc/nginx/ssl:ro
    networks:
      - backend
    depends_on:
      - auth
      - users
      - city
      - core
      - notification
      - scheduler
      - integration
      - admin
      # - terminal  # FUTURE SERVICE - Uncomment when activating terminal service
    healthcheck:
      test: ['CMD-SHELL', 'wget --spider -q http://localhost/health || curl -f http://localhost/health || exit 1']
      interval: 30s
      timeout: 10s
      retries: 3

  auth:
    build:
      context: .
      dockerfile: infra/shared/Dockerfile
      target: production
      args:
        APP_NAME: auth
    container_name: heidi-auth
    restart: always
    env_file:
      - .env
    environment:
      NODE_ENV: production
      SERVICE_NAME: auth
      SERVICE_PORT: 3001
    ports:
      - '3001:3001'
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    networks:
      - backend
    healthcheck:
      test: ['CMD-SHELL', 'wget --spider -q http://localhost:3001/healthz || curl -f http://localhost:3001/healthz || exit 1']
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  users:
    build:
      context: .
      dockerfile: infra/shared/Dockerfile
      target: production
      args:
        APP_NAME: users
    container_name: heidi-users
    restart: always
    env_file:
      - .env
    environment:
      NODE_ENV: production
      SERVICE_NAME: users
      SERVICE_PORT: 3002
    ports:
      - '3002:3002'
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    networks:
      - backend
    healthcheck:
      test: ['CMD-SHELL', 'wget --spider -q http://localhost:3002/healthz || curl -f http://localhost:3002/healthz || exit 1']
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  city:
    build:
      context: .
      dockerfile: infra/shared/Dockerfile
      target: production
      args:
        APP_NAME: city
    container_name: heidi-city
    restart: always
    env_file:
      - .env
    environment:
      NODE_ENV: production
      SERVICE_NAME: city
      SERVICE_PORT: 3003
    ports:
      - '3003:3003'
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    networks:
      - backend
    healthcheck:
      test: ['CMD-SHELL', 'wget --spider -q http://localhost:3003/healthz || curl -f http://localhost:3003/healthz || exit 1']
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  core:
    build:
      context: .
      dockerfile: infra/shared/Dockerfile
      target: production
      args:
        APP_NAME: core
    container_name: heidi-core
    restart: always
    env_file:
      - .env
    environment:
      NODE_ENV: production
      SERVICE_NAME: core
      SERVICE_PORT: 3004
    ports:
      - '3004:3004'
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    networks:
      - backend
    healthcheck:
      test: ['CMD-SHELL', 'wget --spider -q http://localhost:3004/healthz || curl -f http://localhost:3004/healthz || exit 1']
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  notification:
    build:
      context: .
      dockerfile: infra/shared/Dockerfile
      target: production
      args:
        APP_NAME: notification
    container_name: heidi-notification
    restart: always
    env_file:
      - .env
    environment:
      NODE_ENV: production
      SERVICE_NAME: notification
      SERVICE_PORT: 3005
    ports:
      - '3005:3005'
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    networks:
      - backend
    healthcheck:
      test: ['CMD-SHELL', 'wget --spider -q http://localhost:3005/healthz || curl -f http://localhost:3005/healthz || exit 1']
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  scheduler:
    build:
      context: .
      dockerfile: infra/shared/Dockerfile
      target: production
      args:
        APP_NAME: scheduler
    container_name: heidi-scheduler
    restart: always
    env_file:
      - .env
    environment:
      NODE_ENV: production
      SERVICE_NAME: scheduler
      SERVICE_PORT: 3006
    ports:
      - '3006:3006'
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    networks:
      - backend
    healthcheck:
      test: ['CMD-SHELL', 'wget --spider -q http://localhost:3006/healthz || curl -f http://localhost:3006/healthz || exit 1']
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  integration:
    build:
      context: .
      dockerfile: infra/shared/Dockerfile
      target: production
      args:
        APP_NAME: integration
    container_name: heidi-integration
    restart: always
    env_file:
      - .env
    environment:
      NODE_ENV: production
      SERVICE_NAME: integration
      SERVICE_PORT: 3007
    ports:
      - '3007:3007'
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    networks:
      - backend
    healthcheck:
      test: ['CMD-SHELL', 'wget --spider -q http://localhost:3007/healthz || curl -f http://localhost:3007/healthz || exit 1']
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  admin:
    build:
      context: .
      dockerfile: infra/shared/Dockerfile
      target: production
      args:
        APP_NAME: admin
    container_name: heidi-admin
    restart: always
    env_file:
      - .env
    environment:
      NODE_ENV: production
      SERVICE_NAME: admin
      SERVICE_PORT: 3008
    ports:
      - '3008:3008'
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    networks:
      - backend
    healthcheck:
      test: ['CMD-SHELL', 'wget --spider -q http://localhost:3008/healthz || curl -f http://localhost:3008/healthz || exit 1']
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # FUTURE SERVICE - Terminal microservice for managing city terminals
  # To activate: docker compose --profile future up terminal
  # Or uncomment this profile line and include terminal in depends_on
  terminal:
    profiles: ["future"]
    build:
      context: .
      dockerfile: infra/shared/Dockerfile
      target: production
      args:
        APP_NAME: terminal
    container_name: heidi-terminal
    restart: always
    env_file:
      - .env
    environment:
      NODE_ENV: production
      SERVICE_NAME: terminal
      SERVICE_PORT: 3009
      TERMINAL_DATABASE_URL: postgresql://${POSTGRES_USER:-heidi}:${POSTGRES_PASSWORD:-heidi_password}@postgres:5432/heidi_terminal?schema=public
      DATABASE_URL: postgresql://${POSTGRES_USER:-heidi}:${POSTGRES_PASSWORD:-heidi_password}@postgres:5432/heidi_terminal?schema=public
      REDIS_URL: redis://:${REDIS_PASSWORD:-}@redis:6379
      RABBITMQ_URL: amqp://${RABBITMQ_USER:-heidi}:${RABBITMQ_PASSWORD:-heidi_password}@rabbitmq:5672
      JWT_SECRET: ${JWT_SECRET}
      JWT_REFRESH_SECRET: ${JWT_REFRESH_SECRET}
    ports:
      - '3009:3009'
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    networks:
      - backend
    healthcheck:
      test: ['CMD-SHELL', 'wget --spider -q http://localhost:3009/healthz || curl -f http://localhost:3009/healthz || exit 1']
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

# Volumes section removed - using bind mounts to ./data/ directory instead
# This ensures data persists on the host filesystem and is easier to backup
# Data is stored in: ./data/{service-name}/
# volumes:
#   (no named volumes needed - using bind mounts)

networks:
  backend:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.28.0.0/16
